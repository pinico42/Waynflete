\documentclass {article}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{subfiles}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{appendix}
\usepackage{blindtext}
\usepackage{bm}
\usepackage{cleveref}
\usepackage{hhline}
\usepackage{listings}
\usepackage[bb=boondox]{mathalfa}
\usepackage{multirow, bigdelim}
\usepackage{tabu}
\usepackage{yfonts}


\title{Did WW2 Produce an Equivalent to a Modern Computer?}
\date{February 23, 2017}
\author{Max Penrose}


\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}

\begin{document}
\pagenumbering{roman}
\maketitle
\tableofcontents
\clearpage

\section*{Introduction}
Computation rules the modern world - from the internet to the rise of automated labour, today's humanity puts colossal value in the hands of computers, taking them for granted as the remarkable machines they are. However, the fundamental ideas of computation and calculation lying alongside the transistors and silicon at the heart of this revolution are no less remarkable in themselves, primarily down to their simplicity. Furthermore, it is difficult for any modern computer user to see how basic electronics can operate the tasks a computer can perform. Therefore, I feel the natural way to shed light on this question is to look at the earliest and simplest computers, what they could do and what was distinctive about them.

The most significant problem in answering this question is the actual definition of `computer'. In the early 20th Century, prior to developments in this field, any human which evaluated statements through computation was known, logically, as a `computer'. As such, we must explore the qualities that an entity - human, machine or other - must have to be able to perform computation.

\textit{(Leibniz's ideas of computation, all effectively computable statements could be expressed in terms of a simple language \& a method to evaluate these statements - highlighting ideas of solving problems)}

Therefore, in order for a machine to be called a `computer', it must be able to evaluate any member of a set of computable functions. However, this is an abstract notion - to clarify it, we must define a simple language to define computable functions, and find a simple method to evaluate them. We shall explore the two most common methods, the $\lambda$-Calculus and the Turing Machine, and evaluate them in terms of their similarities and differences, before exploring how well this definition is fulfilled by early computers.



\section{Church's $\lambda$-Calculus}

\subfile{"Section 1 - Lambda Calculus"}

\section{Turing Machines}

\subfile{"Section 2 - Turing Machines"}

\section{Universality}

\subfile{"Section 3 - Universality"}

\subfile{"Appendices"}


\bibliography{bibliography}
\bibliographystyle{unsrt}
\end{document}